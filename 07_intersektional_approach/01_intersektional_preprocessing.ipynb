{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import itertools\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open('preprocessed_umfrage.pkl','rb')\n",
    "df = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = df[['UserID','AbiEltern','eigSprache','Sex__m', 'Sex__w','Buecher', 'Erstloesung','Erfolg','Schwierigkeit','Wochentag','ist_Schulzeit','MehrfachFalsch','Testposition__FT', 'Testposition__nt', 'Testposition__pruefung',\n",
    "       'Testposition__training', 'Testposition__version', 'Testposition__vt',\n",
    "       'Testposition__zt', 'beendet', 'Fehler', 'HA__HA', 'HA__Self', 'HA__nt',\n",
    "       'HA__vt', 'HA__zt', 'Klassenstufe', 'Jahredabei']]\n",
    "df_corr = df_corr.drop_duplicates()\n",
    "df_corr = df_corr.drop(columns=['UserID'])\n",
    "df_corr['AbiEltern'] = np.where((df_corr.AbiEltern == '2'),'1',df_corr.AbiEltern)\n",
    "df_corr.AbiEltern = df_corr.AbiEltern.astype('float')\n",
    "df_corr.Buecher = df_corr.Buecher.astype('float')\n",
    "\n",
    "c = df_corr.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('correlationsNewModel.xlsx', engine='xlsxwriter')\n",
    "c.to_excel(writer, sheet_name='corr')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Correlations only Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = df[['UserID','AbiEltern','eigSprache','Sex__m', 'Sex__w','Buecher']]\n",
    "df_corr = df_corr.drop_duplicates()\n",
    "df_corr = df_corr.drop(columns=['UserID'])\n",
    "df_corr['AbiEltern'] = np.where((df_corr.AbiEltern == '2'),'1',df_corr.AbiEltern)\n",
    "df_corr.AbiEltern = df_corr.AbiEltern.astype('float')\n",
    "c = df_corr.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#High degree: If the coefficient value lies between ± 0.50 and ± 1, then it is said to be a strong correlation. \n",
    "# Moderate degree: If the value lies between ± 0.30 and ± 0.49, then it is said to be a medium correlation. \n",
    "# Low degree: When the value lies below + . 29, then it is said to be a small correlation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = sns.heatmap(c, annot=True, cmap=\"Blues\", fmt='.1g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subgrouping for intersektional analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### SUBGROUPING\n",
    "\n",
    "# Abi eltern\n",
    "df.AbiEltern = df.AbiEltern.astype('float')\n",
    "df_abi = df[df.AbiEltern > 0]\n",
    "df_keinAbi = df[df.AbiEltern ==0]\n",
    "\n",
    "#Gender\n",
    "df_boys = df[df.Sex__m == 1]\n",
    "df_girls = df[df.Sex__w == 1]\n",
    "\n",
    "# Migration\n",
    "df_deutsch = df[df.eigSprache == 1]\n",
    "df_migration = df[df.eigSprache == 0]\n",
    "\n",
    "# Anzahl Bücher\n",
    "df['Buecher'] = df['Buecher'].replace(['10'],0)\n",
    "df['Buecher'] = df['Buecher'].replace(['200'],1)\n",
    "df_buch0 = df[df.Buecher == 0.0]\n",
    "df_buch1 = df[df.Buecher == 1]\n",
    "\n",
    "\n",
    "# # # Abi Eltern mit Migration\n",
    "# df_abi_deutsch = df_abi[df_abi.eigSprache == 1]\n",
    "# df_abi_migration = df_abi[df_abi.eigSprache == 0]\n",
    "# df_keinAbi_migration = df_keinAbi[df_keinAbi.eigSprache == 0]\n",
    "# df_keinAbi_deutsch = df_keinAbi[df_keinAbi.eigSprache == 1]\n",
    "\n",
    "# # # Abi Eltern mit Gender\n",
    "# df_abi_w = df_abi[df_abi.Sex__w == 1]\n",
    "# df_abi_m =df_abi[df_abi.Sex__m == 1]\n",
    "# df_keinAbi_w= df_keinAbi[df_keinAbi.Sex__w==1]\n",
    "# df_keinAbi_m = df_keinAbi[df_keinAbi.Sex__m==1]\n",
    "\n",
    "# # Abi Eltern mit Bücher\n",
    "df_abi_buch0 = df_abi[df_abi.Buecher == 0.0]\n",
    "df_abi_buch1 = df_abi[df_abi.Buecher == 1]\n",
    "df_keinAbi_buch0 = df_keinAbi[df_keinAbi.Buecher == 0.0]\n",
    "df_keinAbi_buch1 = df_keinAbi[df_keinAbi.Buecher == 1]\n",
    "\n",
    "# Gender mit Migration\n",
    "# df_w_migration = df_girls[df_girls.eigSprache == 0]\n",
    "# df_w_deutsch = df_girls[df_girls.eigSprache == 1]\n",
    "# df_m_migration = df_boys[df_boys.eigSprache == 0]\n",
    "# df_m_deutsch = df_boys[df_boys.eigSprache == 1]\n",
    "\n",
    "# # Gender mit Bücher\n",
    "# df_w_buch0 = df_girls[df_girls.Buecher == 0.0]\n",
    "# df_w_buch1 = df_girls[df_girls.Buecher == 1]\n",
    "df_m_buch0 = df_boys[df_boys.Buecher == 0.0]\n",
    "df_m_buch1 = df_boys[df_boys.Buecher == 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Migration mit Bücher\n",
    "df_migration_buch0 = df_migration[df_migration.Buecher == 0.0]\n",
    "df_migration_buch1 = df_migration[df_migration.Buecher == 1]\n",
    "# df_deutsch_buch0 = df_deutsch[df_deutsch.Buecher == 0.0]\n",
    "# df_deutsch_buch1 = df_deutsch[df_deutsch.Buecher == 1]\n",
    "\n",
    "all_intersections = [\n",
    "# df_abi_deutsch,\n",
    "# df_abi_migration,\n",
    "# df_keinAbi_migration,\n",
    "# df_keinAbi_deutsch,\n",
    "# df_abi_w,\n",
    "# df_abi_m,\n",
    "# df_keinAbi_w,\n",
    "# df_keinAbi_m,\n",
    "# df_abi_buch0,\n",
    "# df_abi_buch1,\n",
    "# df_keinAbi_buch0,\n",
    "# df_keinAbi_buch1,###\n",
    "# df_w_migration,\n",
    "# df_w_deutsch,\n",
    "# df_m_migration, \n",
    "# df_m_deutsch,\n",
    "# df_w_buch0,\n",
    "# df_w_buch1,\n",
    "df_m_buch0,\n",
    "df_m_buch1,##\n",
    "df_migration_buch0,\n",
    "df_migration_buch1,\n",
    "# df_deutsch_buch0,\n",
    "# df_deutsch_buch1 \n",
    "]\n",
    "\n",
    "all_intersections_string = [\n",
    "# 'df_abi_deutsch',\n",
    "# 'df_abi_migration',\n",
    "# 'df_keinAbi_migration',\n",
    "# 'df_keinAbi_deutsch',\n",
    "# 'df_abi_w',\n",
    "# 'df_abi_m',\n",
    "# 'df_keinAbi_w',\n",
    "# 'df_keinAbi_m',\n",
    "# 'df_abi_buch0',\n",
    "# 'df_abi_buch1',\n",
    "# 'df_keinAbi_buch0',\n",
    "# 'df_keinAbi_buch1',\n",
    "# 'df_w_migration',\n",
    "# 'df_w_deutsch',\n",
    "# 'df_m_migration', \n",
    "# 'df_m_deutsch',\n",
    "# 'df_w_buch0',\n",
    "# 'df_w_buch1',\n",
    "'df_m_buch0',\n",
    "'df_m_buch1',\n",
    "'df_migration_buch0',\n",
    "'df_migration_buch1',\n",
    "# 'df_deutsch_buch0',\n",
    "# 'df_deutsch_buch1' \n",
    "]\n",
    "\n",
    "#for x in all_intersections:\n",
    "    #print(str(x))\n",
    "    #print(len(x.UserID.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold , cross_val_score\n",
    "from sklearn import metrics \n",
    "from matplotlib import pyplot as plt\n",
    "from numpy import mean\n",
    "from numpy import absolute\n",
    "import openpyxl\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import log_loss, roc_auc_score, recall_score, precision_score, accuracy_score, plot_roc_curve, plot_confusion_matrix, roc_curve, confusion_matrix\n",
    "from matplotlib import pyplot\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import log_loss, roc_auc_score, recall_score, precision_score, average_precision_score, f1_score, classification_report, accuracy_score, plot_roc_curve, plot_precision_recall_curve, plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import feature list\n",
    "infile = open('../04_predictionUserHistory/01_data/FINALsmallSampleSet_3months_without_duplicates.pkl','rb')\n",
    "import_file = pickle.load(infile)\n",
    "infile.close()\n",
    "df_f = import_file\n",
    "feature_cols = list(df_f.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(clf,X,y):\n",
    "    pred = clf.predict(X)\n",
    "    a = accuracy_score(y,pred)\n",
    "    p = precision_score(y,pred)\n",
    "    r = recall_score(y,pred)\n",
    "    roc_auc = roc_auc_score(y,pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y, pred).ravel()\n",
    "    fpr = fp/(fp+tn)\n",
    "\n",
    "    return a,p,r,roc_auc,fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DECISION TREE\n",
    "fair_metrics = pd.DataFrame(columns=['model', 'group', 'Accuracy', 'Precision', 'Recall', 'AUC', 'FPR'])\n",
    "\n",
    "DTE_model = pickle.load(open('../04_predictionUserHistory/02_decisionTree/DecisionTreemodel_3months.pkl', 'rb'))\n",
    "\n",
    "for (all_intersections, all_intersections_string) in zip(all_intersections, all_intersections_string):\n",
    "    df = all_intersections\n",
    "    dataset= df[df.columns[df.columns.isin(feature_cols)]]\n",
    "    y = dataset['Erfolg']\n",
    "    X = dataset.drop(columns=['Erfolg'])\n",
    "    a,p,r,roc_auc,fpr = get_metrics(DTE_model,X,y)\n",
    "    fair_metrics = fair_metrics.append({'model':'DTE','group':all_intersections_string,'Accuracy':a,'Precision': p, 'Recall':r, 'AUC':roc_auc, 'FPR':fpr}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fair_metrics.to_excel('intersectional_DTEmetrics.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "logreg_model = pickle.load(open('../04_predictionUserHistory/03_logisticRegression/Logregmodel_3months.pkl', 'rb'))\n",
    "\n",
    "for (all_intersections, all_intersections_string) in zip(all_intersections, all_intersections_string):\n",
    "    df = all_intersections\n",
    "\n",
    "    dataset = df[df.columns[df.columns.isin(feature_cols)]]\n",
    "    y = dataset['Erfolg']\n",
    "    X = dataset.drop(columns=['Erfolg'])\n",
    "\n",
    "    a,p,r,roc_auc,fpr = get_metrics(logreg_model,X,y)\n",
    "    fair_metrics = fair_metrics.append({'model':'LogReg','group':all_intersections_string,'Accuracy':a,'Precision': p, 'Recall':r, 'AUC':roc_auc, 'FPR':fpr}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fair_metrics.to_excel('intersectional_LRmetrics.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "svm_model = pickle.load(open('../04_predictionUserHistory/04_svm/SVMmodel_3months.pkl', 'rb'))\n",
    "\n",
    "for (all_intersections, all_intersections_string) in zip(all_intersections, all_intersections_string):\n",
    "    df = all_intersections\n",
    "\n",
    "    dataset = df[df.columns[df.columns.isin(feature_cols)]]\n",
    "    y = dataset['Erfolg']\n",
    "    X = dataset.drop(columns=['Erfolg'])\n",
    "    a,p,r,roc_auc,fpr = get_metrics(svm_model,X,y)\n",
    "    fair_metrics = fair_metrics.append({'model':'SVM','group':all_intersections_string,'Accuracy':a,'Precision': p, 'Recall':r, 'AUC':roc_auc, 'FPR':fpr}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fair_metrics.to_excel('intersectional_SVMmetrics.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN\n",
    "fair_metrics = pd.DataFrame(columns=['model', 'group', 'Accuracy', 'Precision', 'Recall', 'AUC', 'FPR'])\n",
    "\n",
    "nn_model = load_model('../04_predictionUserHistory/05_nn/nn_3months/')\n",
    "\n",
    "def get_dn_metrics(model, X,y):\n",
    "    X = np.asarray(X).astype('float32')\n",
    "    yhat_probs = model.predict(X, verbose=0)\n",
    "    yhat_classes =  (model.predict(X) > 0.5).astype(\"int32\")\n",
    "    # reduce to 1d array\n",
    "    yhat_probs = yhat_probs[:, 0]\n",
    "    yhat_classes = yhat_classes[:, 0]\n",
    "    a = accuracy_score(y, yhat_classes)\n",
    "    p = precision_score(y, yhat_classes)\n",
    "    r = recall_score(y, yhat_classes)\n",
    "    roc_auc = roc_auc_score(y, yhat_probs)\n",
    "    tn, fp, fn, tp = confusion_matrix(y, yhat_classes).ravel()\n",
    "    fpr = fp/(fp+tn)\n",
    "\n",
    "    return a,p,r,roc_auc,fpr\n",
    "\n",
    "for (all_intersections, all_intersections_string) in zip(all_intersections, all_intersections_string):\n",
    "    df = all_intersections\n",
    "\n",
    "    dataset = df[df.columns[df.columns.isin(feature_cols)]]\n",
    "    y = dataset['Erfolg']\n",
    "    X = dataset.drop(columns=['Erfolg'])\n",
    "\n",
    "    a,p,r,roc_auc,fpr = get_dn_metrics(nn_model,X,y)\n",
    "    fair_metrics = fair_metrics.append({'model':'NN','group':all_intersections_string,'Accuracy':a,'Precision': p, 'Recall':r, 'AUC':roc_auc, 'FPR':fpr}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fair_metrics.to_excel('intersectional_NNmetrics6.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fair_metrics.to_excel('intersectional_ALLmetrics.xlsx')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
